{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 時間目\n",
    "# フィルタを使って学習機械を作ってみよう\n",
    "\n",
    "それでは，最後の時間では，現代 AI の核心技術となりつつあるフィルタを使ったネットワーク，畳み込みニューラルネットワーク (Convolution Neural Network) を実現してみましょう．\n",
    "\n",
    "１時間目と２時間目では，視覚システムにおいて畳み込み（フィルタ）演算が重要な役割を担っていそうということを学びました．\n",
    "ただし，フィルタを設計するには，なんらかの指針が必要でした．\n",
    "\n",
    "サルやヒトの初期視覚システムの研究では，微分型のような特徴抽出するフィルタと，局所的な積分形のようなフィルタが組み合わさって構成されていそうだということがわかっています．ただし，実際に特徴抽出となると，その設計は難しくなってしまいます．\n",
    "そこで，フィルタの係数は，データから決めましょうという考え方が出てきました．このようにデータから（フィルタの係数のような）パラメータを決めることを　*学習* などと呼びます．\n",
    "\n",
    "ここでは，\n",
    "\n",
    "1. フィルタの枠組みを設計して\n",
    "2. 得られた表現を，判別するような学習機械をつなぎ合わせ\n",
    "3. データからの学習によって，全体のパラメータを調整する\n",
    "\n",
    "という方法を取っていくこととします．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先程までは `numpy` や `scipy` といった，科学技術演算パッケージを用いてきましたが，ここでは *機械学習/深層学習* をメインに取り扱うため，そのために設計された `keras` と呼ばれるパッケージを用います．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# とりあえずはおまじない\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "import os # MNIST をダウンロードするため proxy を設定\n",
    "os.environ[\"http_proxy\"] = \"http://proxy.uec.ac.jp:8080/\"\n",
    "os.environ[\"https_proxy\"] = \"http://proxy.uec.ac.jp:8080/\"\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データを用意する．\n",
    "\n",
    "ここでは手始めに MNIST と呼ばれる数字文字のデータセットを使って実験を行います．\n",
    "MNIST は 28x28 画素という比較的小さい画像の中に手書きの数字が書かれたデータセットです．ただしその画像枚数は多く，70,000枚が内包されています．\n",
    "\n",
    "よく使うやり方は，60,000 枚のデータをパラメータの学習に用いて，残りの 10,000 枚は，できた機械がどの程度の性能を持つかをテストするために用います．以下では，\n",
    "\n",
    "* `x_train` に学習用の数字データ\n",
    "* `y_train` に上記学習データのラベル （'0'から '9' までの値）\n",
    "* `x_test` にテスト用の数字データ\n",
    "* `y_test` に上記テストデータのラベル\n",
    "\n",
    "としてデータを用います．\n",
    "\n",
    "データ自体は keras の中に用意されています．`keras.datasets` の中にある `mnist` モジュールを使うと便利です．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train_label), (x_test, y_test_label) = mnist.load_data()\n",
    "\n",
    "# このままだと扱いにくいので画像データは [0, 1] の値を持つデータにします．\n",
    "x_train = x_train.astype('float32').reshape(60000, 28, 28, 1)\n",
    "x_test = x_test.astype('float32').reshape(10000, 28, 28, 1)\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# ラベルデータもちょっと扱いにくいので 10 要素のベクトルで表す (one-hot vector 表現)\n",
    "# この表現は，\n",
    "# ラベルが '0' であれば [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "# ラベルが '1' であれば [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "# ラベルが '2' であれば [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "# といった表現になる\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train_label)\n",
    "y_test = keras.utils.to_categorical(y_test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 試しに最初の 9 個くらいを描画してみる\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "for i in range(9):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    plt.imshow(x_train[i, :, :, 0], cmap='gray')\n",
    "    plt.title(y_train_label[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 視覚モデルを構築してみる．\n",
    "\n",
    "まずは，初期視覚野的なモデルを考えてみましょう．初期視覚野のモデルは，いくつかの特徴抽出を行うフィルタと，それぞれをぼかすフィルタから成り立っています．ここでは，適当に入力画像を 32 種類のフィルタで分解しておき，それぞれをぼかすような形の枠組みを考えてみます．\n",
    "`model` という変数に作っていきます．\n",
    "\n",
    "とりあえず，\n",
    "\n",
    "* 特徴抽出のフィルタは 5x5 とし，32種類，\n",
    "* 特徴抽出後は正の値のみを通す (Rectified Linear (ReLU)と呼ばれる演算です)\n",
    "* ぼかす操作(MaxPoolingが対応)は，フィルタで 2x2 の範囲をぼかすような形で作ってみることにします\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(16, kernel_size=(3, 3), input_shape=(28, 28, 1)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここまでで，視覚に使われているような表現になっていると思うことにしましょう．\n",
    "次に，この表現を分類器にかけてみます．分類器も `model` に追加していくことで一つの学習機械を定義することができます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分類器，とりあえずはおまじないと思っておｋ\n",
    "\n",
    "model.add(Flatten()) # 上記表現を1列のベクトルに並べて\n",
    "model.add(Dense(10, activation='softmax')) # 識別器を構成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの学習\n",
    "\n",
    "さて，ここまででモデルができました．でもフィルタの係数とかは決まってません．この部分を学習で決めることにします．\n",
    "学習自体は，ある種の関数(loss関数と呼ばれます)を小さくすることで求めます．\n",
    "ここでは `categorical_crossentropy` と呼ばれるを loss 関数としています．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "\n",
    "# 学習モデルを定義して\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "# 学習させます\n",
    "history = model.fit(x_train, y_train, batch_size=128, epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss の値\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('loss value')\n",
    "plt.grid()\n",
    "\n",
    "\n",
    "# 学習データに対する正答率\n",
    "plt.figure()\n",
    "plt.plot(history.history['acc'])\n",
    "plt.title('Accuracy (For training set)')\n",
    "plt.grid()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習データセットに関しては 99 % 程度の精度で識別できていることがわかります．．\n",
    "次にテストセットで評価してみましょう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テストセットで評価してみる\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "\n",
    "print('test loss: ', score[0])\n",
    "print('test accuracy: ', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "おおよそ 98 % 程度の精度（が出るはず）であるので，まぁまぁのものができたと思われます．\n",
    "\n",
    "ついでに，間違えたやつをピックアップしてみましょう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 識別間違いしたやつをピックアップしてみる\n",
    "y_pred_label = model.predict_classes(x_test)\n",
    "\n",
    "idx = (y_pred_label != y_test_label) # 答えが違う添字のやつを抜き出し\n",
    "x_failed = x_test[idx]\n",
    "y_true_failed = y_test_label[idx]\n",
    "y_pred_failed = y_pred_label[idx]\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "for i in range(9):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    plt.imshow(x_failed[i, :, :, 0], cmap='gray')\n",
    "    plt.title('True: %d, Pred: %d' % (y_true_failed[i], y_pred_failed[i]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
